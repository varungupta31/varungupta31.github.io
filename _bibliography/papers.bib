---
---

@article{guptacvelociti,
  title={VELOCITI: Benchmarking Video-Language Compositional Reasoning with Strict Entailment},
  author={S*, Darshana and <b>Gupta*</b>, <b>Varun</b> and Singh*, Darshan and Khan, Zeeshan and Gandhi, Vineet and Tapaswi, Makarand},
  journal={Computer Vision and Pattern Recognition (CVPR),},
  year={2025},
  abstract = {A fundamental aspect of compositional reasoning in a video is associating people and their actions across time. Recent years have seen great progress in general-purpose vision/video models and a move towards long-video understanding. While exciting, we take a step back and ask: are todayâ€™s models good at compositional reasoning on short videos? To this end, we introduce VELOCITI, a benchmark to study Video-LLMs by disentangling and assessing the comprehension of agents, actions, and their associations across multiple events. We adopt the Video-Language Entailment setup and propose StrictVLE that requires correct classification (rather than ranking) of the positive and negative caption. We evaluate several models and observe that even the best, LLaVA-OneVision (42.5%) and GPT-4o (44.3%), are far from human accuracy at 89.6%. Results show that action understanding lags behind agents, and negative captions created using entities appearing in the video perform worse than those obtained from pure text manipulation. We also present challenges with ClassicVLE and multiple-choice (MC) evaluation, strengthening our preference for StrictVLE. Finally, we validate that our benchmark requires visual inputs of multiple frames making it ideal to study video-language compositional reasoning.},
  code={https://github.com/katha-ai/VELOCITI},
  arxiv={https://arxiv.org/abs/2406.10889},
  preview={velociti.png},
  }

@article{guptac4mts,
  title={C4MTS: Challenge on Categorizing Missing Traffic Signs from Contextual Cues},
  author={<b>Gupta</b>, <b>Varun</b> and Subramanian, Anbumani and Jawahar, C.V, and Saluja, Rohit, and Others*},
  journal={Lecture Notes in Electrical Engineering, Springer Nature, },
  year={2024},
  preview={c4mts.png},
}

@article{guptaICRA,
  title={CueCAn: Cue Driven Contextual Attention Units for Identifying Missing Traffic Signs on Unconstrained Roads},
  author={<b>Gupta</b>, <b>Varun</b> and Subramanian, Anbumani and Jawahar, C.V, and Saluja, Rohit},
  journal={IEEE International Conference on Robotics and Automation (ICRA), },
  year={2023},
  arxiv={https://arxiv.org/abs/2303.02641},
  code={https://github.com/iHubData-Mobility/public-CueCAn},
  preview={cuecan.gif},
}

@article{gupta2021infrared,
  title={Infrared Thermography and Computer Vision Based Human Respiration Monitoring},
  author={<b>Gupta</b>, <b>Varun</b> and Jagadev, Preeti and Giri, Lalat Indu},
  journal={International Symposium of Asian Control Association on Intelligent Robotics and Industrial Automation (IRIA), },
  year={2021},
  pdf={https://ieeexplore.ieee.org/document/9588754/},
  preview={thermalRes.gif},
  
  
}

@article{gupta2021thermal,
  title={Thermal Nostril Tracking Using YOLOv4-Tiny},
  author={<b>Gupta</b>, <b>Varun</b> and Jagadev, Preeti and Giri, Lalat Indu},
  journal={International Conference on Technology, Research, and Innovation for Betterment of Society (TRIBES), },
  year={2021},
  pdf={https://ieeexplore.ieee.org/document/9751624},
  preview={respiration.gif},
  
}